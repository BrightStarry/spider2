2016年12月19日 20:25:42
	昨晚算是完成了一个基本能够实现的宽度优先的爬虫。
	但我被一个问题困恼了很久，就是爬取的每个网页都要先用httpClient请求获取response
中的输入流，然后写入文件，但是下面要解析这个html的时候，又要用jsoup根据URL再次建立一个
连接，再次获取到那个html，再次提取出其中的链接。这样效率很低。而我之前想到的解决办法，就是
从读取已经保存的html文给jsoup解析。但这样好像也不太好。
	这个项目我的想法是 使用 redis进行队列存储  ，然后 获取response中的content后直接
转成字符串，用jsoup解析出  待访问的链接  以及  其中的图片链接  ，然后最好还有一个线程从redis
中读取图片链接，下载到本地。 如果此时redis中没有数据，那么可以先暂停1s、这样。
	好，开始动手。
------------------------------------------------------------------------
被一个傻逼的问题困恼了半天，就是使用jedis.jar的连接池配置，结果new 出对象后死活调用不了
想用的方法，而我看到源码里明明有那个方法，后来百度了很久无意间才看见，解析jedis中pool的源码
竟然需要Commons Pool.jar 可怕。。
注意 Commons-pool.jar的版本也不能低
------------------------------------------------------------------------
最新版本（2.9（2.4+））的jedis返还到连接池的方法，是redis.close
-------------------------------------------------------------------
java7 新特性 try-with-resource语句
这个语句确保在语句最后每个资源都被关闭，任何实现了java.lang.autoCloseable的对象，
包括实现了java.io.closeable的对象，都可以用作一个资源。
看了看，就这么写   try(BufferReader br = xxx){}catch(xx){}finally{}
注意，所有资源都会在catch和finally之前关闭
------------------------------------------------------------------------
昨晚后来发现redis的list并不好用，因为如果要去重，只能先把全部数据取出来，在java中用list遍历一遍
，无意中看见redis 有一个sorted set，那就用他吧。
------------------------------------------------------------------------


-------------------------------------------
2016年12月21日 15:47:50
 	再记录一下，现在有有点问题。
 	我目前是这样的想法，就是从女优列表开始爬取。先把所有 女优主页 爬取出来  然后再从女优主页 爬取出 所有高清的影片页面  最后把这个影片页面相关内容
 解析一下，保存起来。
 	其中。保存最后的结果，用的是redis 的hash，（要把实体类序列化，然后key值用md5） 然后待爬取列表，用sorted set。问题是现在，因为最先两
 步骤爬取下来的都不是要保存的URL，那么不好用hash存，嗯。我突然想到了。已爬取的队列其实只要能方便查询就好了。只要已爬取的页面中没有，那么往待爬取
 的sorted set中插入的时候会自动去重的。
 	好像也不对。这样吧。试下直接用key/value的形式存字符，把爬取过的url以MD5的形式存到redis的key中。value为空就好了。查询的时候只要试着
 查询那个key，如果没有就ok。
 -----------------------------------------------------------------------------------------------------
 我忽然感觉我的异常处理写的很烂。我基本上把每个类的方法全都try-catch了。是不是应该不每个都try，直接throw个自己包装的异常，到了service
 统一处理是不是更好。
 还有一点，就是。从连接池中取redis的连接，如果是单线程，那么是不是不应该取得那么频繁。？例如每个循环开始取出一个连接，循环结束，返回去
 -------------------------------------------------------------------------------
 commons-io包有inputStream 转 string，并可以设置编码
 
 
 ------------------------------------------------------------------------------------------
 2016年12月21日 19:15:18
 	写的时差不多了。但好像有个很大的问题。那就是。。。已爬取队列似乎没用了。。。
 	因为，待爬取的队列是sorted set的，而我这个爬虫，因为它的特殊性，是把所有要爬取的url都入队之后在进行爬取的。
 就完全没有重复爬取的可能性了。。。我日哦
 ------------------------------------------------------------------------------------------------
 2016年12月21日 21:55:33  
 	我真的要好好说下这个爬虫。。。太可怕了。
 	晚上爬的差不多了。基本上就只剩最后一个问题，就是这个网站的磁力链接是发送ajax请求过去获取的。我找了半天，才找到这个js。然后把这个js
 格式化了一下看了看。是发了一个ajax的。但是有几个参数，有一个gid死活不知道从哪冒出来的。我把整个网页保存到本地。再打开，然后把格式化之后
 的js复制到原来的js上， 这样才可以方便的下断点。（我就是这样找到这个ajax的参数什么的）。
 	后来。。我才发现  他的好几个参数就明明白白的写在页面上。。。。。我日了它老母。。。。。。。
 -------------------------------------------------------------------------------------------------
 对了。好像还有一个http请求的工具   htmlunit  听说比httpClient好用
 -----------------------------------------------------------------------
 上面那个ajax请求，获取不到返回回来的json，虽然是成功的。但只返回了一句js。用来把一个div隐藏掉。而没有磁力链接的json数据
 （我现在想了想，是不是我找错了js方法？）
 算了。我现在再弄刚才说的的htmlunit。听说这个可以完全的模拟浏览器的操作，比如点击按钮什么的。就相当于一个没有界面的浏览器。
 然后当有js什么的时候，还可以自动等待它加载完成，获取加载完成后的页面、
 不管怎么样，先试一下了。
 --------------------------------------------------------------------------------------
 昨晚就弄了很久，11点多了才上传，刚刚起床又弄了下。终于成功了。使用htmlunit 模拟浏览器。主要就是要
 1.设置启用js  2.设置支持ajax的controller  3.设置等待后台js加载的时间。还有超时时间  4.也可以设置一个循环。每次执行page的wait()方法（！同步的）
 这样。基本就可以获取到执行完js之后的界面了。（听说用htmlunit可以直接模拟浏览器进行登录操作）
 另外注意：
代码如下：
		//模拟一个浏览器  谷歌浏览器
		WebClient webClient = new WebClient(BrowserVersion.CHROME);//设置User-Agent(也就是你用什么浏览器)
		webClient.getOptions().setThrowExceptionOnScriptError(false); //脚本运行错误时是否抛出异常
	    webClient.getOptions().setThrowExceptionOnFailingStatusCode(false);  //失败的状态码时是否抛出异常
	    webClient.getOptions().setJavaScriptEnabled(true);  //js是否加载
        webClient.getOptions().setActiveXNative(false);  //？ 这个好像是加载flash的控件
        webClient.getOptions().setCssEnabled(false);  //css是否加载
        
        webClient.setAjaxController(new NicelyResynchronizingAjaxController());//设置支持ajax
        webClient.getOptions().setTimeout(30000);//设置“浏览器”的请求超时时间
        webClient.getOptions().setRedirectEnabled(true);//是否启用重定向
        
        //打开网页
        HtmlPage page = (HtmlPage)webClient.getPage("https://www.javbus.co/CPDE-005");
        webClient.waitForBackgroundJavaScript(600*1000);  //设置JS后台等待执行时间，这个好像是要在获取页面之后设置才有效
        webClient.setJavaScriptTimeout(10000);//设置JS执行的超时时间 。一样。都是在获取页面之后设置的
        
        //在这个等待的循环中，别人好像都执行了一些解析页面元素的方法。。。而我是使用jsoup进行解析的，。所以只能够等待了
        /*for(int i=0;i <20; i++){
        	 synchronized (page) {
                 page.wait(500);
            }
        }*/
        
        String html = page.asXml();
        System.out.println(html);
        webClient.close();
如果报这种错  java.lang.NoClassDefFoundError 从官网把所有相关的jar也都烤过来
 --------------------------------------------------------------------------------------------
 很多jar中。例如commons-io  commons-lang  中有很多方便的轮子
 ------------------------------------------------------------------------------------------------
 2016年12月22日 12:28:14
	基本应该可以了。现在好像就还一个问题。。那就是每个女优主页。。也还是有分页的。。
	看了下。和女优列表的分页差不多。也是后面 /x  这样判断页数的。。
--------------------------------------------------------------------------
2016年12月22日 13:48:09
	基本已经差不多了。就是还没有下载图片什么的。小问题了。就是这个网站下载的真的有点慢。
	。对了。我可以把url这个实体类直接保存成html的形式，这样打开是不是直接就可以自动下载图片了。
	我原先还想着把图片都下载下来，保存成word。
----------------------------------------------------------------------------------
如果按照索引遍历一个集合，然后要在中间删除某个元素。导致后面的遍历下标越界。可以试试下标倒序遍历。
---------------------------------------------------------------------------------
2016年12月22日 15:47:34
	目前已经再爬了。稳定。没问题。全部保存成html的格式。
	就是爬取每个影片的时候需要等待ajax获取到磁力链接才可以，所以爬得是真的慢。
-----------------------------------------------------------------------